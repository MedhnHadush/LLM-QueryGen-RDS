{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfSJOsiPX3id"
      },
      "outputs": [],
      "source": [
        "# Importing Google Drive module from Google Colab\n",
        "from google.colab import drive\n",
        "\n",
        "# Mounting Google Drive to access its contents in the Colab environment\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r \"requirement.txt\" -q"
      ],
      "metadata": {
        "id": "cFwzMMZWYtfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile model.py\n",
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments\n",
        ")\n",
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer\n",
        "from huggingface_hub import login\n",
        "import dotenv\n",
        "\n",
        "class Train:\n",
        "    \"\"\"\n",
        "    A class to handle the training of a LLaMA model for a specific task.\n",
        "\n",
        "    Attributes:\n",
        "    ----------\n",
        "    data : str\n",
        "        The dataset identifier from Hugging Face hub.\n",
        "    train_size : str\n",
        "        The portion of the dataset to be used for training.\n",
        "    model_id : str\n",
        "        The identifier of the pre-trained model to be fine-tuned.\n",
        "    new_model : str\n",
        "        The directory to save the newly trained model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.data = \"b-mc2/sql-create-context\"\n",
        "        self.train_size = \"train[:10]\"  # Picking first 10 rows for faster training\n",
        "        self.model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "        self.new_model = \"./llama-2-7b-chat-v2\"\n",
        "\n",
        "    def combine_fields(self, example):\n",
        "        \"\"\"\n",
        "        Combines multiple fields from the dataset into a single 'text' field.\n",
        "\n",
        "        Parameters:\n",
        "        ----------\n",
        "        example : dict\n",
        "            A dictionary containing the dataset fields.\n",
        "\n",
        "        Returns:\n",
        "        -------\n",
        "        dict\n",
        "            A dictionary with a combined 'text' field.\n",
        "        \"\"\"\n",
        "        example['text'] = f\"Context: {example['context']} Question: {example['question']} Answer: {example['answer']}\"\n",
        "        return example\n",
        "\n",
        "    def train_llama(self):\n",
        "        \"\"\"\n",
        "        Trains a LLaMA model using the specified dataset and configurations.\n",
        "        \"\"\"\n",
        "        # Load environment variables from .env file\n",
        "        dotenv.load_dotenv('./Variables.env')\n",
        "\n",
        "        # Login to Hugging Face Hub using the API key from environment variables\n",
        "        login(token=os.environ.get('hf_key'))\n",
        "\n",
        "        # Load and preprocess the dataset\n",
        "        dataset = load_dataset(self.data, split=self.train_size)\n",
        "        dataset = dataset.map(self.combine_fields)\n",
        "        dataset = dataset.remove_columns(['answer', 'question', 'context'])\n",
        "\n",
        "        # Define model configurations\n",
        "        base_model_id = self.model_id\n",
        "        compute_dtype = getattr(torch, \"float16\")\n",
        "\n",
        "        quant_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=compute_dtype,\n",
        "            bnb_4bit_use_double_quant=False,\n",
        "        )\n",
        "\n",
        "        # Load pre-trained model with quantization\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            base_model_id,\n",
        "            quantization_config=quant_config,\n",
        "            device_map={\"\": 0}\n",
        "        )\n",
        "        model.config.use_cache = False\n",
        "        model.config.pretraining_tp = 1\n",
        "\n",
        "        # Load tokenizer\n",
        "        tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True)\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        tokenizer.padding_side = \"right\"\n",
        "\n",
        "        # Define PEFT (Parameter-Efficient Fine-Tuning) parameters\n",
        "        peft_params = LoraConfig(\n",
        "            lora_alpha=16,\n",
        "            lora_dropout=0.1,\n",
        "            r=64,\n",
        "            bias=\"none\",\n",
        "            task_type=\"CAUSAL_LM\"\n",
        "        )\n",
        "\n",
        "        # Define training parameters\n",
        "        training_params = TrainingArguments(\n",
        "            output_dir=\"./results\",\n",
        "            num_train_epochs=1,\n",
        "            per_device_train_batch_size=4,\n",
        "            gradient_accumulation_steps=8,\n",
        "            optim=\"paged_adamw_32bit\",\n",
        "            save_steps=25,\n",
        "            logging_steps=25,\n",
        "            learning_rate=2e-4,\n",
        "            weight_decay=0.001,\n",
        "            fp16=True,  # Enable mixed precision training\n",
        "            bf16=False,\n",
        "            max_grad_norm=0.3,\n",
        "            max_steps=-1,\n",
        "            warmup_ratio=0.03,\n",
        "            group_by_length=True,\n",
        "            lr_scheduler_type=\"constant\",\n",
        "            report_to=\"tensorboard\"\n",
        "        )\n",
        "\n",
        "        # Initialize the trainer\n",
        "        trainer = SFTTrainer(\n",
        "            model=model,\n",
        "            train_dataset=dataset,\n",
        "            peft_config=peft_params,\n",
        "            dataset_text_field=\"text\",\n",
        "            max_seq_length=None,\n",
        "            tokenizer=tokenizer,\n",
        "            args=training_params,\n",
        "            packing=False,\n",
        "        )\n",
        "\n",
        "        # Train the model\n",
        "        trainer.train()\n",
        "\n",
        "        # Save the trained model\n",
        "        trainer.save_model(self.new_model)\n"
      ],
      "metadata": {
        "id": "ytLAiJ2nX5ZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import model\n",
        "modelsample = model.Train()\n",
        "modelsample.train_llama()"
      ],
      "metadata": {
        "id": "3zK-dhvyY4gU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import psycopg2\n",
        "from psycopg2 import sql\n",
        "from dotenv import load_dotenv\n",
        "from langchain import PromptTemplate\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    pipeline\n",
        ")\n",
        "from huggingface_hub import login\n",
        "import pandas as pd\n",
        "from sqlalchemy import create_engine, text\n",
        "import logging\n",
        "import re\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "import os\n",
        "import dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "dotenv.load_dotenv('./.env')\n",
        "\n",
        "# Function to create a database connection\n",
        "def create_connection():\n",
        "    user = os.environ.get('user')\n",
        "    password = os.environ.get('password')\n",
        "    host = os.environ.get('host')\n",
        "    port = os.environ.get('port')  # Default port for PostgreSQL is 5432\n",
        "    database = os.environ.get('database')\n",
        "\n",
        "    connection_string = f'postgresql://{user}:{password}@{host}:{port}/{database}'\n",
        "    logging.debug(f'Connecting to database with connection string: {connection_string}')\n",
        "    engine = create_engine(connection_string)\n",
        "    return engine.connect()\n",
        "\n",
        "# Function to execute a SQL query and return the result as a DataFrame\n",
        "def execute_query(query):\n",
        "    try:\n",
        "        conn = create_connection()\n",
        "        logging.debug(f'Executing query: {query}')\n",
        "        df = pd.read_sql_query(text(query), conn)\n",
        "        conn.close()\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        logging.error(f'Error executing query: {e}')\n",
        "        return None\n",
        "\n",
        "# Streamlit page configuration\n",
        "st.set_page_config(page_title=\"Streamlit SQL Chatbot\", page_icon=\"ðŸ¤–\")\n",
        "st.title(\"SQL Chatbot\")\n",
        "\n",
        "# Function to generate a SQL query based on user input\n",
        "def get_response(user_query):\n",
        "    login(token=os.environ.get('hf_key'))\n",
        "\n",
        "    prompt = user_query\n",
        "    print(\"Prompt:\", prompt)\n",
        "    model_name = \"./llama-2-7b-chat-v2\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
        "\n",
        "    result = pipe(f\"<s>[INST]<<sys>>I want you to act as a SQL query expert. Convert this sentence: {prompt} : into PostgreSQL query <</sys>>[/INST]\")\n",
        "    generated_text = result[0]['generated_text']\n",
        "\n",
        "    sql_query = extract_sql_query(generated_text)\n",
        "    print(\"Generated Text:\", generated_text)\n",
        "    print(\"SQL Query:\", sql_query)\n",
        "    return sql_query\n",
        "\n",
        "# Function to extract SQL query from generated text\n",
        "def extract_sql_query(input_text):\n",
        "    sql_query_match = re.search(r'```(?:sql\\n)?(.*?)\\n```', input_text, re.DOTALL)\n",
        "    if sql_query_match:\n",
        "        sql_query = sql_query_match.group(1).strip()\n",
        "    else:\n",
        "        sql_query = \"Unable to extract SQL query.\"\n",
        "    return sql_query\n",
        "\n",
        "# Initialize session state for chat history and DataFrames\n",
        "if 'chat_history' not in st.session_state:\n",
        "    st.session_state.chat_history = [AIMessage(content=\"Hello, I am a bot. How can I help you?\")]\n",
        "if 'dataframes' not in st.session_state:\n",
        "    st.session_state.dataframes = [None]  # Initialize with None for the initial bot message\n",
        "\n",
        "# Display chat history\n",
        "for idx, message in enumerate(st.session_state.chat_history):\n",
        "    if isinstance(message, AIMessage):\n",
        "        with st.chat_message(\"AI\"):\n",
        "            st.write(message.content)\n",
        "            if st.session_state.dataframes[idx] is not None:\n",
        "                st.dataframe(st.session_state.dataframes[idx])\n",
        "    elif isinstance(message, HumanMessage):\n",
        "        with st.chat_message(\"Human\"):\n",
        "            st.write(message.content)\n",
        "\n",
        "# User input\n",
        "user_query = st.chat_input(\"Type your message here...\")\n",
        "if user_query is not None and user_query != \"\":\n",
        "    st.session_state.chat_history.append(HumanMessage(content=user_query))\n",
        "\n",
        "    with st.chat_message(\"Human\"):\n",
        "        st.markdown(user_query)\n",
        "\n",
        "    with st.chat_message(\"AI\"):\n",
        "        response = get_response(user_query)\n",
        "        df = execute_query(response)\n",
        "        st.write(response)\n",
        "        st.dataframe(df)\n",
        "\n",
        "        # Store the response and DataFrame in the session state\n",
        "        st.session_state.chat_history.append(AIMessage(content=response))\n",
        "        st.session_state.dataframes.append(None)\n",
        "        st.session_state.dataframes.append(df)\n"
      ],
      "metadata": {
        "id": "mvpyuHStZTWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: Replace [authtoken] with your actual ngrok authtoken\n",
        "!ngrok authtoken [authtoken]\n",
        "\n",
        "# Start ngrok tunnel on port 8501 (default for Streamlit)\n",
        "public_url = ngrok.connect(8501)\n",
        "\n",
        "# Print the public URL generated by ngrok\n",
        "print('Public URL:', public_url)\n",
        "\n",
        "# Run the Streamlit app in the background\n",
        "# Note: 'app.py' should be your Streamlit application script\n",
        "!streamlit run app.py &>/dev/null&"
      ],
      "metadata": {
        "id": "hEXYRmfhZ8mC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}